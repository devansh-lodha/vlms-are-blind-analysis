{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import Qwen2VLForConditionalGeneration, PaliGemmaForConditionalGeneration, MllamaForConditionalGeneration, AutoModelForCausalLM, LlavaNextProcessor, LlavaNextForConditionalGeneration, AutoProcessor, AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gt</th>\n",
       "      <th>linewidth</th>\n",
       "      <th>resolution</th>\n",
       "      <th>distances</th>\n",
       "      <th>image_path</th>\n",
       "      <th>prompt</th>\n",
       "      <th>Qwen/Qwen2-VL-7B-Instruct</th>\n",
       "      <th>google/paligemma-3b-pt-448</th>\n",
       "      <th>microsoft/Florence-2-large</th>\n",
       "      <th>meta-llama/Llama-3.2-11B-Vision-Instruct</th>\n",
       "      <th>llava-hf/llava-v1.6-mistral-7b-hf</th>\n",
       "      <th>OpenGVLab/InternVL2_5-8B-MPO</th>\n",
       "      <th>microsoft/Phi-3.5-vision-instruct</th>\n",
       "      <th>mistralai/Pixtral-12B-2409</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gt_1_image_0_thickness_2_resolution_384</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>[1.0, 10.0, 0.0]</td>\n",
       "      <td>./my2DlinePlots/gt_1_image_0_thickness_2_resol...</td>\n",
       "      <td>How many times do the blue and red lines touch...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gt_1_image_0_thickness_4_resolution_384</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>[1.0, 10.0, 0.0]</td>\n",
       "      <td>./my2DlinePlots/gt_1_image_0_thickness_4_resol...</td>\n",
       "      <td>Count the intersection points where the blue a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gt_1_image_0_thickness_2_resolution_768</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>[1.0, 10.0, 0.0]</td>\n",
       "      <td>./my2DlinePlots/gt_1_image_0_thickness_2_resol...</td>\n",
       "      <td>How many times do the blue and red lines touch...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gt_1_image_0_thickness_4_resolution_768</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>[1.0, 10.0, 0.0]</td>\n",
       "      <td>./my2DlinePlots/gt_1_image_0_thickness_4_resol...</td>\n",
       "      <td>Count the intersection points where the blue a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gt_1_image_0_thickness_2_resolution_1152</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>[1.0, 10.0, 0.0]</td>\n",
       "      <td>./my2DlinePlots/gt_1_image_0_thickness_2_resol...</td>\n",
       "      <td>How many times do the blue and red lines touch...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>gt_2_image_299_thickness_4_resolution_384</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>[4.0, 6.0, 1.0]</td>\n",
       "      <td>./my2DlinePlots/gt_2_image_299_thickness_4_res...</td>\n",
       "      <td>Count the intersection points where the blue a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>gt_2_image_299_thickness_2_resolution_768</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>[4.0, 6.0, 1.0]</td>\n",
       "      <td>./my2DlinePlots/gt_2_image_299_thickness_2_res...</td>\n",
       "      <td>How many times do the blue and red lines touch...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>gt_2_image_299_thickness_4_resolution_768</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>[4.0, 6.0, 1.0]</td>\n",
       "      <td>./my2DlinePlots/gt_2_image_299_thickness_4_res...</td>\n",
       "      <td>Count the intersection points where the blue a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>gt_2_image_299_thickness_2_resolution_1152</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>[4.0, 6.0, 1.0]</td>\n",
       "      <td>./my2DlinePlots/gt_2_image_299_thickness_2_res...</td>\n",
       "      <td>How many times do the blue and red lines touch...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>gt_2_image_299_thickness_4_resolution_1152</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>[4.0, 6.0, 1.0]</td>\n",
       "      <td>./my2DlinePlots/gt_2_image_299_thickness_4_res...</td>\n",
       "      <td>Count the intersection points where the blue a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        filename  gt  linewidth  resolution  \\\n",
       "0        gt_1_image_0_thickness_2_resolution_384   1          2         100   \n",
       "1        gt_1_image_0_thickness_4_resolution_384   1          4         100   \n",
       "2        gt_1_image_0_thickness_2_resolution_768   1          2         200   \n",
       "3        gt_1_image_0_thickness_4_resolution_768   1          4         200   \n",
       "4       gt_1_image_0_thickness_2_resolution_1152   1          2         300   \n",
       "...                                          ...  ..        ...         ...   \n",
       "3595   gt_2_image_299_thickness_4_resolution_384   2          4         100   \n",
       "3596   gt_2_image_299_thickness_2_resolution_768   2          2         200   \n",
       "3597   gt_2_image_299_thickness_4_resolution_768   2          4         200   \n",
       "3598  gt_2_image_299_thickness_2_resolution_1152   2          2         300   \n",
       "3599  gt_2_image_299_thickness_4_resolution_1152   2          4         300   \n",
       "\n",
       "             distances                                         image_path  \\\n",
       "0     [1.0, 10.0, 0.0]  ./my2DlinePlots/gt_1_image_0_thickness_2_resol...   \n",
       "1     [1.0, 10.0, 0.0]  ./my2DlinePlots/gt_1_image_0_thickness_4_resol...   \n",
       "2     [1.0, 10.0, 0.0]  ./my2DlinePlots/gt_1_image_0_thickness_2_resol...   \n",
       "3     [1.0, 10.0, 0.0]  ./my2DlinePlots/gt_1_image_0_thickness_4_resol...   \n",
       "4     [1.0, 10.0, 0.0]  ./my2DlinePlots/gt_1_image_0_thickness_2_resol...   \n",
       "...                ...                                                ...   \n",
       "3595   [4.0, 6.0, 1.0]  ./my2DlinePlots/gt_2_image_299_thickness_4_res...   \n",
       "3596   [4.0, 6.0, 1.0]  ./my2DlinePlots/gt_2_image_299_thickness_2_res...   \n",
       "3597   [4.0, 6.0, 1.0]  ./my2DlinePlots/gt_2_image_299_thickness_4_res...   \n",
       "3598   [4.0, 6.0, 1.0]  ./my2DlinePlots/gt_2_image_299_thickness_2_res...   \n",
       "3599   [4.0, 6.0, 1.0]  ./my2DlinePlots/gt_2_image_299_thickness_4_res...   \n",
       "\n",
       "                                                 prompt  \\\n",
       "0     How many times do the blue and red lines touch...   \n",
       "1     Count the intersection points where the blue a...   \n",
       "2     How many times do the blue and red lines touch...   \n",
       "3     Count the intersection points where the blue a...   \n",
       "4     How many times do the blue and red lines touch...   \n",
       "...                                                 ...   \n",
       "3595  Count the intersection points where the blue a...   \n",
       "3596  How many times do the blue and red lines touch...   \n",
       "3597  Count the intersection points where the blue a...   \n",
       "3598  How many times do the blue and red lines touch...   \n",
       "3599  Count the intersection points where the blue a...   \n",
       "\n",
       "      Qwen/Qwen2-VL-7B-Instruct  google/paligemma-3b-pt-448  \\\n",
       "0                            -1                          -1   \n",
       "1                            -1                          -1   \n",
       "2                            -1                          -1   \n",
       "3                            -1                          -1   \n",
       "4                            -1                          -1   \n",
       "...                         ...                         ...   \n",
       "3595                         -1                          -1   \n",
       "3596                         -1                          -1   \n",
       "3597                         -1                          -1   \n",
       "3598                         -1                          -1   \n",
       "3599                         -1                          -1   \n",
       "\n",
       "      microsoft/Florence-2-large  meta-llama/Llama-3.2-11B-Vision-Instruct  \\\n",
       "0                             -1                                        -1   \n",
       "1                             -1                                        -1   \n",
       "2                             -1                                        -1   \n",
       "3                             -1                                        -1   \n",
       "4                             -1                                        -1   \n",
       "...                          ...                                       ...   \n",
       "3595                          -1                                        -1   \n",
       "3596                          -1                                        -1   \n",
       "3597                          -1                                        -1   \n",
       "3598                          -1                                        -1   \n",
       "3599                          -1                                        -1   \n",
       "\n",
       "      llava-hf/llava-v1.6-mistral-7b-hf  OpenGVLab/InternVL2_5-8B-MPO  \\\n",
       "0                                    -1                            -1   \n",
       "1                                    -1                            -1   \n",
       "2                                    -1                            -1   \n",
       "3                                    -1                            -1   \n",
       "4                                    -1                            -1   \n",
       "...                                 ...                           ...   \n",
       "3595                                 -1                            -1   \n",
       "3596                                 -1                            -1   \n",
       "3597                                 -1                            -1   \n",
       "3598                                 -1                            -1   \n",
       "3599                                 -1                            -1   \n",
       "\n",
       "      microsoft/Phi-3.5-vision-instruct  mistralai/Pixtral-12B-2409  \n",
       "0                                    -1                          -1  \n",
       "1                                    -1                          -1  \n",
       "2                                    -1                          -1  \n",
       "3                                    -1                          -1  \n",
       "4                                    -1                          -1  \n",
       "...                                 ...                         ...  \n",
       "3595                                 -1                          -1  \n",
       "3596                                 -1                          -1  \n",
       "3597                                 -1                          -1  \n",
       "3598                                 -1                          -1  \n",
       "3599                                 -1                          -1  \n",
       "\n",
       "[3600 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./my2DlinePlots/metadata.json\", 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(metadata, orient='index')\n",
    "\n",
    "df = df.reset_index().rename(columns={'index': 'filename'})\n",
    "\n",
    "df['image_path'] = df['filename'].apply(lambda x: os.path.join(\"./my2DlinePlots\", x + \".png\"))\n",
    "\n",
    "df = df.drop(columns=['grid_size'])\n",
    "\n",
    "# Add prompt column\n",
    "prompts = {\n",
    "    \"prompt1\": \"How many times do the blue and red lines touch each other? Answer with a number in curly brackets, e.g., {5}.\",\n",
    "    \"prompt2\": \"Count the intersection points where the blue and red lines meet. Put your answer in curly brackets, e.g., {2}.\"\n",
    "}\n",
    "\n",
    "# Duplicate rows for each prompt\n",
    "data = pd.concat([df, df], ignore_index=True)\n",
    "data['prompt'] = [prompts[\"prompt1\"] if i % 2 == 0 else prompts[\"prompt2\"] for i in range(len(data))]\n",
    "\n",
    "\n",
    "model_names = [\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    \"google/paligemma-3b-pt-448\",\n",
    "    \"microsoft/Florence-2-large\",\n",
    "    \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    \"OpenGVLab/InternVL2_5-8B-MPO\",\n",
    "    \"microsoft/Phi-3.5-vision-instruct\",\n",
    "    \"mistralai/Pixtral-12B-2409\"\n",
    "]\n",
    "\n",
    "for model_name in model_names:\n",
    "    data[model_name] = -1\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction(model, processor, image, prompt):\n",
    "    if model_name == \"Qwen/Qwen2-VL-7B-Instruct\":\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Preprocess the inputs\n",
    "        text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        # Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "        inputs = processor(\n",
    "            text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "\n",
    "        # Inference: Generation of the output\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=16)\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids) :]\n",
    "            for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "        ]\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return output_text.strip(\"[]{}\")\n",
    "    \n",
    "    elif model_name == \"google/paligemma-3b-pt-448\":\n",
    "        # Process inputs and move to MPS\n",
    "        model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        # Generate output\n",
    "        with torch.inference_mode():\n",
    "            generation = model.generate(**model_inputs, max_new_tokens=16, do_sample=False)\n",
    "            generation = generation[0][input_len:]\n",
    "            decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "            return decoded\n",
    "    \n",
    "    elif model_name == \"microsoft/Florence-2-large\":\n",
    "        # Process inputs and move to MPS\n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\"mps\", torch.float16)\n",
    "\n",
    "        # Generate output\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=16,\n",
    "            num_beams=3,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "        # Decode and post-process the output\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "        parsed_answer = processor.post_process_generation(generated_text, task=prompt, image_size=(image.width, image.height))\n",
    "        return parsed_answer\n",
    "    \n",
    "    elif model_name == \"meta-llama/Llama-3.2-11B-Vision-Instruct\":\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]}\n",
    "        ]\n",
    "        input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = processor(\n",
    "            image,\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        output = model.generate(**inputs, max_new_tokens=32)\n",
    "        raw = processor.decode(output[0])\n",
    "        return raw\n",
    "    \n",
    "    elif model_name == \"llava-hf/llava-v1.6-mistral-7b-hf\":\n",
    "        conversation = [\n",
    "            {\n",
    "\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        prompt_processed = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "        inputs = processor(images=image, text=prompt_processed, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "        # autoregressively complete prompt\n",
    "        output = model.generate(**inputs, max_new_tokens=16)\n",
    "\n",
    "        out = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        match = re.search(r\"{(\\d+)}\", out)\n",
    "        return int(match.group(1)) if match else out\n",
    "    \n",
    "    elif model_name == \"OpenGVLab/InternVL2_5-8B-MPO\":\n",
    "        IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "        IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "        def build_transform(input_size):\n",
    "            MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "            transform = T.Compose([\n",
    "                T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "                T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=MEAN, std=STD)\n",
    "            ])\n",
    "            return transform\n",
    "\n",
    "        def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "            best_ratio_diff = float('inf')\n",
    "            best_ratio = (1, 1)\n",
    "            area = width * height\n",
    "            for ratio in target_ratios:\n",
    "                target_aspect_ratio = ratio[0] / ratio[1]\n",
    "                ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "                if ratio_diff < best_ratio_diff:\n",
    "                    best_ratio_diff = ratio_diff\n",
    "                    best_ratio = ratio\n",
    "                elif ratio_diff == best_ratio_diff:\n",
    "                    if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                        best_ratio = ratio\n",
    "            return best_ratio\n",
    "\n",
    "        def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "            orig_width, orig_height = image.size\n",
    "            aspect_ratio = orig_width / orig_height\n",
    "\n",
    "            # calculate the existing image aspect ratio\n",
    "            target_ratios = set(\n",
    "                (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "                i * j <= max_num and i * j >= min_num)\n",
    "            target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "            # find the closest aspect ratio to the target\n",
    "            target_aspect_ratio = find_closest_aspect_ratio(\n",
    "                aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "            # calculate the target width and height\n",
    "            target_width = image_size * target_aspect_ratio[0]\n",
    "            target_height = image_size * target_aspect_ratio[1]\n",
    "            blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "            # resize the image\n",
    "            resized_img = image.resize((target_width, target_height))\n",
    "            processed_images = []\n",
    "            for i in range(blocks):\n",
    "                box = (\n",
    "                    (i % (target_width // image_size)) * image_size,\n",
    "                    (i // (target_width // image_size)) * image_size,\n",
    "                    ((i % (target_width // image_size)) + 1) * image_size,\n",
    "                    ((i // (target_width // image_size)) + 1) * image_size\n",
    "                )\n",
    "                # split the image\n",
    "                split_img = resized_img.crop(box)\n",
    "                processed_images.append(split_img)\n",
    "            assert len(processed_images) == blocks\n",
    "            if use_thumbnail and len(processed_images) != 1:\n",
    "                thumbnail_img = image.resize((image_size, image_size))\n",
    "                processed_images.append(thumbnail_img)\n",
    "            return processed_images\n",
    "\n",
    "        def load_image(image, input_size=448, max_num=12):\n",
    "            transform = build_transform(input_size=input_size)\n",
    "            images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "            pixel_values = [transform(image) for image in images]\n",
    "            pixel_values = torch.stack(pixel_values)\n",
    "            return pixel_values\n",
    "        \n",
    "        \n",
    "        width, _ = image.size\n",
    "        if width <= 384:  # 384x384\n",
    "            num = 1\n",
    "        elif width <= 768:  # 768x768\n",
    "            num = 4\n",
    "        else:  # 1152x1152\n",
    "            num = 9\n",
    "\n",
    "        # set the max number of tiles in `max_num`\n",
    "        pixel_values = load_image(image, max_num=num)\n",
    "        pixel_values = pixel_values.to(\"cpu\", torch.float16)\n",
    "        generation_config = dict(max_new_tokens=16, do_sample=True)\n",
    "    \n",
    "        question = '<image>\\n' + prompt\n",
    "        response = model.chat(processor, pixel_values, question, generation_config)\n",
    "        return response.strip(\"[]{}\")\n",
    "\n",
    "    elif model_name == \"microsoft/Phi-3.5-vision-instruct\":    \n",
    "        message = [\n",
    "            {\"role\": \"user\", \"content\": f\"<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n\"},\n",
    "        ]\n",
    "\n",
    "        prompt = processor.tokenizer.apply_chat_template(\n",
    "        message, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cpu\") \n",
    "\n",
    "        generation_args = { \n",
    "            \"max_new_tokens\": 16, \n",
    "            \"temperature\": 0.0, \n",
    "            \"do_sample\": False, \n",
    "        } \n",
    "\n",
    "        generate_ids = model.generate(**inputs, \n",
    "        eos_token_id=processor.tokenizer.eos_token_id, \n",
    "        **generation_args\n",
    "        ).to(\"cpu\")\n",
    "\n",
    "        # remove input tokens \n",
    "        generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "        response = processor.batch_decode(generate_ids, \n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "        return response.strip(\"[]{}\")\n",
    "    \n",
    "    elif model_name == \"mistralai/Pixtral-12B-2409\":\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": prompt}, {\"type\": \"image\", \"image\": image}]\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        outputs = model.chat(messages, sampling_params=processor)\n",
    "\n",
    "        return outputs[0].outputs[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_huggingface_token():\n",
    "    # Define the path to the token file (updated path)\n",
    "    token_file = Path.home() / \".cache\" / \"huggingface\" / \"token\"\n",
    "\n",
    "    # Check if the token file exists\n",
    "    if token_file.exists():\n",
    "        with open(token_file, \"r\") as file:\n",
    "            token = file.read().strip()\n",
    "            return token\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Hugging Face token file not found. Please run 'huggingface-cli login'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    if model_name == \"Qwen/Qwen2-VL-7B-Instruct\":\n",
    "        model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_name, torch_dtype=\"auto\", device_map=\"auto\"\n",
    "        )\n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    elif model_name == \"google/paligemma-3b-pt-448\":\n",
    "        model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"mps\",\n",
    "            revision=\"float16\",  # Use float16 revision for better compatibility\n",
    "            token = get_huggingface_token()\n",
    "        ).eval()\n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    elif model_name == \"microsoft/Florence-2-large\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True\n",
    "        ).to(\"mps\")\n",
    "\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "   \n",
    "    elif model_name == \"meta-llama/Llama-3.2-11B-Vision-Instruct\":\n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    elif model_name == \"llava-hf/llava-v1.6-mistral-7b-hf\":\n",
    "        processor = LlavaNextProcessor.from_pretrained(model_name)\n",
    "        model = LlavaNextForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True) \n",
    "        model.to(\"mps\")\n",
    "\n",
    "    elif model_name == \"OpenGVLab/InternVL2_5-8B-MPO\":\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        ).to('cpu').eval()\n",
    "    \n",
    "        processor = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "    elif model_name == \"microsoft/Phi-3.5-vision-instruct\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            device_map=\"cpu\", \n",
    "            trust_remote_code=True, \n",
    "            torch_dtype=torch.float16, \n",
    "            _attn_implementation='eager',\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "\n",
    "        # for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "        processor = AutoProcessor.from_pretrained(model_name, \n",
    "        trust_remote_code=True, \n",
    "        num_crops=16\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"mistralai/Pixtral-12B-2409\":\n",
    "        processor = SamplingParams(max_tokens=16)\n",
    "        model = LLM(model=model_name, tokenizer_mode=\"mistral\", device=\"cpu\")\n",
    "\n",
    "    # Iterate through the DataFrame rows\n",
    "    for index, row in tqdm(data.iterrows(), total=len(data), desc=f\"{model_name}\"):\n",
    "        image_path = row['image_path']\n",
    "        prompt = row['prompt']\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Get the model prediction for the image and prompt using model specific inference\n",
    "        prediction = get_model_prediction(model, processor, image, prompt) \n",
    "\n",
    "        # Store the prediction in the DataFrame\n",
    "        data.loc[index, model_name] = prediction\n",
    "\n",
    "    # delete model and free memory\n",
    "    del model\n",
    "    del processor\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after data is done we can run metrics using the dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
