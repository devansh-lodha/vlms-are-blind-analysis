{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import Qwen2VLForConditionalGeneration, PaliGemmaForConditionalGeneration, MllamaForConditionalGeneration, AutoModelForCausalLM, LlavaNextProcessor, LlavaNextForConditionalGeneration, AutoProcessor, AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import gc\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gt</th>\n",
       "      <th>linewidth</th>\n",
       "      <th>resolution</th>\n",
       "      <th>distances</th>\n",
       "      <th>image_path</th>\n",
       "      <th>prompt</th>\n",
       "      <th>Qwen/Qwen2-VL-7B-Instruct</th>\n",
       "      <th>meta-llama/Llama-3.2-11B-Vision-Instruct</th>\n",
       "      <th>llava-hf/llava-v1.6-mistral-7b-hf</th>\n",
       "      <th>OpenGVLab/InternVL2_5-8B-MPO</th>\n",
       "      <th>microsoft/Phi-3.5-vision-instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gt_1_image_0_thickness_2_resolution_384</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>[1.0, 10.0, 0.0]</td>\n",
       "      <td>./my2DlinePlots/gt_1_image_0_thickness_2_resol...</td>\n",
       "      <td>How many times do the blue and red lines touch...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gt_1_image_0_thickness_4_resolution_384</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>[1.0, 10.0, 0.0]</td>\n",
       "      <td>./my2DlinePlots/gt_1_image_0_thickness_4_resol...</td>\n",
       "      <td>Count the intersection points where the blue a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gt_1_image_0_thickness_2_resolution_768</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>[1.0, 10.0, 0.0]</td>\n",
       "      <td>./my2DlinePlots/gt_1_image_0_thickness_2_resol...</td>\n",
       "      <td>How many times do the blue and red lines touch...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gt_1_image_0_thickness_4_resolution_768</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>[1.0, 10.0, 0.0]</td>\n",
       "      <td>./my2DlinePlots/gt_1_image_0_thickness_4_resol...</td>\n",
       "      <td>Count the intersection points where the blue a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gt_1_image_0_thickness_2_resolution_1152</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>[1.0, 10.0, 0.0]</td>\n",
       "      <td>./my2DlinePlots/gt_1_image_0_thickness_2_resol...</td>\n",
       "      <td>How many times do the blue and red lines touch...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>gt_2_image_299_thickness_4_resolution_384</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>[4.0, 6.0, 1.0]</td>\n",
       "      <td>./my2DlinePlots/gt_2_image_299_thickness_4_res...</td>\n",
       "      <td>Count the intersection points where the blue a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>gt_2_image_299_thickness_2_resolution_768</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>[4.0, 6.0, 1.0]</td>\n",
       "      <td>./my2DlinePlots/gt_2_image_299_thickness_2_res...</td>\n",
       "      <td>How many times do the blue and red lines touch...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>gt_2_image_299_thickness_4_resolution_768</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>[4.0, 6.0, 1.0]</td>\n",
       "      <td>./my2DlinePlots/gt_2_image_299_thickness_4_res...</td>\n",
       "      <td>Count the intersection points where the blue a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>gt_2_image_299_thickness_2_resolution_1152</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>[4.0, 6.0, 1.0]</td>\n",
       "      <td>./my2DlinePlots/gt_2_image_299_thickness_2_res...</td>\n",
       "      <td>How many times do the blue and red lines touch...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>gt_2_image_299_thickness_4_resolution_1152</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>[4.0, 6.0, 1.0]</td>\n",
       "      <td>./my2DlinePlots/gt_2_image_299_thickness_4_res...</td>\n",
       "      <td>Count the intersection points where the blue a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        filename  gt  linewidth  resolution  \\\n",
       "0        gt_1_image_0_thickness_2_resolution_384   1          2         100   \n",
       "1        gt_1_image_0_thickness_4_resolution_384   1          4         100   \n",
       "2        gt_1_image_0_thickness_2_resolution_768   1          2         200   \n",
       "3        gt_1_image_0_thickness_4_resolution_768   1          4         200   \n",
       "4       gt_1_image_0_thickness_2_resolution_1152   1          2         300   \n",
       "...                                          ...  ..        ...         ...   \n",
       "3595   gt_2_image_299_thickness_4_resolution_384   2          4         100   \n",
       "3596   gt_2_image_299_thickness_2_resolution_768   2          2         200   \n",
       "3597   gt_2_image_299_thickness_4_resolution_768   2          4         200   \n",
       "3598  gt_2_image_299_thickness_2_resolution_1152   2          2         300   \n",
       "3599  gt_2_image_299_thickness_4_resolution_1152   2          4         300   \n",
       "\n",
       "             distances                                         image_path  \\\n",
       "0     [1.0, 10.0, 0.0]  ./my2DlinePlots/gt_1_image_0_thickness_2_resol...   \n",
       "1     [1.0, 10.0, 0.0]  ./my2DlinePlots/gt_1_image_0_thickness_4_resol...   \n",
       "2     [1.0, 10.0, 0.0]  ./my2DlinePlots/gt_1_image_0_thickness_2_resol...   \n",
       "3     [1.0, 10.0, 0.0]  ./my2DlinePlots/gt_1_image_0_thickness_4_resol...   \n",
       "4     [1.0, 10.0, 0.0]  ./my2DlinePlots/gt_1_image_0_thickness_2_resol...   \n",
       "...                ...                                                ...   \n",
       "3595   [4.0, 6.0, 1.0]  ./my2DlinePlots/gt_2_image_299_thickness_4_res...   \n",
       "3596   [4.0, 6.0, 1.0]  ./my2DlinePlots/gt_2_image_299_thickness_2_res...   \n",
       "3597   [4.0, 6.0, 1.0]  ./my2DlinePlots/gt_2_image_299_thickness_4_res...   \n",
       "3598   [4.0, 6.0, 1.0]  ./my2DlinePlots/gt_2_image_299_thickness_2_res...   \n",
       "3599   [4.0, 6.0, 1.0]  ./my2DlinePlots/gt_2_image_299_thickness_4_res...   \n",
       "\n",
       "                                                 prompt  \\\n",
       "0     How many times do the blue and red lines touch...   \n",
       "1     Count the intersection points where the blue a...   \n",
       "2     How many times do the blue and red lines touch...   \n",
       "3     Count the intersection points where the blue a...   \n",
       "4     How many times do the blue and red lines touch...   \n",
       "...                                                 ...   \n",
       "3595  Count the intersection points where the blue a...   \n",
       "3596  How many times do the blue and red lines touch...   \n",
       "3597  Count the intersection points where the blue a...   \n",
       "3598  How many times do the blue and red lines touch...   \n",
       "3599  Count the intersection points where the blue a...   \n",
       "\n",
       "      Qwen/Qwen2-VL-7B-Instruct  meta-llama/Llama-3.2-11B-Vision-Instruct  \\\n",
       "0                            -1                                        -1   \n",
       "1                            -1                                        -1   \n",
       "2                            -1                                        -1   \n",
       "3                            -1                                        -1   \n",
       "4                            -1                                        -1   \n",
       "...                         ...                                       ...   \n",
       "3595                         -1                                        -1   \n",
       "3596                         -1                                        -1   \n",
       "3597                         -1                                        -1   \n",
       "3598                         -1                                        -1   \n",
       "3599                         -1                                        -1   \n",
       "\n",
       "      llava-hf/llava-v1.6-mistral-7b-hf  OpenGVLab/InternVL2_5-8B-MPO  \\\n",
       "0                                    -1                            -1   \n",
       "1                                    -1                            -1   \n",
       "2                                    -1                            -1   \n",
       "3                                    -1                            -1   \n",
       "4                                    -1                            -1   \n",
       "...                                 ...                           ...   \n",
       "3595                                 -1                            -1   \n",
       "3596                                 -1                            -1   \n",
       "3597                                 -1                            -1   \n",
       "3598                                 -1                            -1   \n",
       "3599                                 -1                            -1   \n",
       "\n",
       "      microsoft/Phi-3.5-vision-instruct  \n",
       "0                                    -1  \n",
       "1                                    -1  \n",
       "2                                    -1  \n",
       "3                                    -1  \n",
       "4                                    -1  \n",
       "...                                 ...  \n",
       "3595                                 -1  \n",
       "3596                                 -1  \n",
       "3597                                 -1  \n",
       "3598                                 -1  \n",
       "3599                                 -1  \n",
       "\n",
       "[3600 rows x 12 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./my2DlinePlots/metadata.json\", 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(metadata, orient='index')\n",
    "\n",
    "df = df.reset_index().rename(columns={'index': 'filename'})\n",
    "\n",
    "df['image_path'] = df['filename'].apply(lambda x: os.path.join(\"./my2DlinePlots\", x + \".png\"))\n",
    "\n",
    "df = df.drop(columns=['grid_size'])\n",
    "\n",
    "# Add prompt column\n",
    "prompts = {\n",
    "    \"prompt1\": \"How many times do the blue and red lines touch each other? Answer with a number in curly brackets, e.g., {5}.\",\n",
    "    \"prompt2\": \"Count the intersection points where the blue and red lines meet. Put your answer in curly brackets, e.g., {2}.\"\n",
    "}\n",
    "\n",
    "# Duplicate rows for each prompt\n",
    "data = pd.concat([df, df], ignore_index=True)\n",
    "data['prompt'] = [prompts[\"prompt1\"] if i % 2 == 0 else prompts[\"prompt2\"] for i in range(len(data))]\n",
    "\n",
    "\n",
    "model_names = [\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    #\"google/paligemma-3b-pt-448\",\n",
    "    \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    \"OpenGVLab/InternVL2_5-8B-MPO\",\n",
    "    \"microsoft/Phi-3.5-vision-instruct\",\n",
    "]\n",
    "\n",
    "for model_name in model_names:\n",
    "    data[model_name] = -1\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction(model, processor, image, prompt):\n",
    "    if model_name == \"Qwen/Qwen2-VL-7B-Instruct\":\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Preprocess the inputs\n",
    "        text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        # Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "        inputs = processor(\n",
    "            text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "\n",
    "        # Inference: Generation of the output\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=16)\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids) :]\n",
    "            for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "        ]\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        # check if output text is list\n",
    "        if isinstance(output_text, list):\n",
    "            if output_text[0].strip(\"[]{}\") == \"\":\n",
    "                return -1\n",
    "            else:\n",
    "                return int(output_text[0].strip(\"[]{}\"))\n",
    "        else:\n",
    "            if output_text.strip(\"[]{}\") == \"\":\n",
    "                return -1\n",
    "            else:\n",
    "                return int(output_text.strip(\"[]{}\"))\n",
    "    \n",
    "    elif model_name == \"google/paligemma-3b-pt-448\":\n",
    "        # Process inputs and move to MPS\n",
    "        model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "        \n",
    "        input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        # Generate output\n",
    "        with torch.inference_mode():\n",
    "            generation = model.generate(**model_inputs, max_new_tokens=16, do_sample=False)\n",
    "            generation = generation[0][input_len:]\n",
    "            decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "            return decoded\n",
    "    \n",
    "    elif model_name == \"meta-llama/Llama-3.2-11B-Vision-Instruct\":\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]}\n",
    "        ]\n",
    "        input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = processor(\n",
    "            image,\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        output = model.generate(**inputs, max_new_tokens=1024)\n",
    "        raw = processor.decode(output[0])\n",
    "        response = raw.split(\"<|start_header_id|>assistant<|end_header_id|>\")[1].split(\"<|eot_id|>\")[0].strip()\n",
    "\n",
    "        # Try to find integers within curly braces first (e.g., \"{2}\")\n",
    "        match = re.search(r\"\\{(\\d+)\\}\", response)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        \n",
    "        # Then try to find standalone integers\n",
    "        match = re.search(r\"\\b(\\d+)\\b\", response) #\\b ensures it is a whole word\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "\n",
    "        # Handle \"twice\", \"two\", \"three\", \"four\", etc.\n",
    "        response = response.lower()\n",
    "        if \"twice\" in response or \"two\" in response:\n",
    "            return 2\n",
    "        elif \"three\" or \"thrice\" in response:\n",
    "            return 3\n",
    "        elif \"four\" in response:\n",
    "            return 4\n",
    "        elif \"one\" or \"once\" in response:\n",
    "            return 1\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    elif model_name == \"llava-hf/llava-v1.6-mistral-7b-hf\":\n",
    "        conversation = [\n",
    "            {\n",
    "\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        prompt_processed = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "        inputs = processor(images=image, text=prompt_processed, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "        # autoregressively complete prompt\n",
    "        output = model.generate(**inputs, max_new_tokens=16, pad_token_id=processor.tokenizer.pad_token_id)\n",
    "\n",
    "        out = processor.decode(output[0], skip_special_tokens=True)\n",
    "        out = out.split(\"[/INST]\")[1].strip().strip(\"{}\")\n",
    "        return int(out)\n",
    "    \n",
    "    elif model_name == \"OpenGVLab/InternVL2_5-8B-MPO\":\n",
    "        IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "        IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "        def build_transform(input_size):\n",
    "            MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "            transform = T.Compose([\n",
    "                T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "                T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=MEAN, std=STD)\n",
    "            ])\n",
    "            return transform\n",
    "\n",
    "        def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "            best_ratio_diff = float('inf')\n",
    "            best_ratio = (1, 1)\n",
    "            area = width * height\n",
    "            for ratio in target_ratios:\n",
    "                target_aspect_ratio = ratio[0] / ratio[1]\n",
    "                ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "                if ratio_diff < best_ratio_diff:\n",
    "                    best_ratio_diff = ratio_diff\n",
    "                    best_ratio = ratio\n",
    "                elif ratio_diff == best_ratio_diff:\n",
    "                    if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                        best_ratio = ratio\n",
    "            return best_ratio\n",
    "\n",
    "        def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "            orig_width, orig_height = image.size\n",
    "            aspect_ratio = orig_width / orig_height\n",
    "\n",
    "            # calculate the existing image aspect ratio\n",
    "            target_ratios = set(\n",
    "                (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "                i * j <= max_num and i * j >= min_num)\n",
    "            target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "            # find the closest aspect ratio to the target\n",
    "            target_aspect_ratio = find_closest_aspect_ratio(\n",
    "                aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "            # calculate the target width and height\n",
    "            target_width = image_size * target_aspect_ratio[0]\n",
    "            target_height = image_size * target_aspect_ratio[1]\n",
    "            blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "            # resize the image\n",
    "            resized_img = image.resize((target_width, target_height))\n",
    "            processed_images = []\n",
    "            for i in range(blocks):\n",
    "                box = (\n",
    "                    (i % (target_width // image_size)) * image_size,\n",
    "                    (i // (target_width // image_size)) * image_size,\n",
    "                    ((i % (target_width // image_size)) + 1) * image_size,\n",
    "                    ((i // (target_width // image_size)) + 1) * image_size\n",
    "                )\n",
    "                # split the image\n",
    "                split_img = resized_img.crop(box)\n",
    "                processed_images.append(split_img)\n",
    "            assert len(processed_images) == blocks\n",
    "            if use_thumbnail and len(processed_images) != 1:\n",
    "                thumbnail_img = image.resize((image_size, image_size))\n",
    "                processed_images.append(thumbnail_img)\n",
    "            return processed_images\n",
    "\n",
    "        def load_image(image, input_size=448, max_num=12):\n",
    "            transform = build_transform(input_size=input_size)\n",
    "            images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "            pixel_values = [transform(image) for image in images]\n",
    "            pixel_values = torch.stack(pixel_values)\n",
    "            return pixel_values\n",
    "\n",
    "        # set the max number of tiles in `max_num`\n",
    "        pixel_values = load_image(image, max_num=12).to(torch.float16).cuda()\n",
    "        generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "        question = '<image>\\n' + prompt\n",
    "        response = model.chat(processor, pixel_values, question, generation_config)\n",
    "        \n",
    "        match = re.search(r\"\\d+\", response)\n",
    "        if match:\n",
    "            return int(match.group())\n",
    "        elif \"one\" in response:\n",
    "            return 1\n",
    "        elif \"two\" in response:\n",
    "            return 2\n",
    "        elif \"three\" in response:\n",
    "            return 3\n",
    "        elif \"not\" in response:\n",
    "            return 0\n",
    "        else:\n",
    "            return response\n",
    "\n",
    "    elif model_name == \"microsoft/Phi-3.5-vision-instruct\":    \n",
    "        message = [\n",
    "            {\"role\": \"user\", \"content\": f\"<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n\"},\n",
    "        ]\n",
    "\n",
    "        prompt = processor.tokenizer.apply_chat_template(\n",
    "        message, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n",
    "\n",
    "        generation_args = { \n",
    "            \"max_new_tokens\": 16, \n",
    "            \"do_sample\": False, \n",
    "        } \n",
    "\n",
    "        generate_ids = model.generate(**inputs, \n",
    "        eos_token_id=processor.tokenizer.eos_token_id, \n",
    "        **generation_args\n",
    "        )\n",
    "\n",
    "        # remove input tokens \n",
    "        generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "        response = processor.batch_decode(generate_ids, \n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "        return int(response.strip(\"{}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_huggingface_token():\n",
    "    # Define the path to the token file (updated path)\n",
    "    token_file = Path.home() / \".cache\" / \"huggingface\" / \"token\"\n",
    "\n",
    "    # Check if the token file exists\n",
    "    if token_file.exists():\n",
    "        with open(token_file, \"r\") as file:\n",
    "            token = file.read().strip()\n",
    "            return token\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Hugging Face token file not found. Please run 'huggingface-cli login'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Qwen/Qwen2-VL-7B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a675c3f42314d03909f4c864fbfea4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8298ff982e47208a825d135961f3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen/Qwen2-VL-7B-Instruct:   0%|          | 0/3600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluating meta-llama/Llama-3.2-11B-Vision-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9884240db74aea8dfcaa54ffed8911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497e32e96636495b9bfadd370935525f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "meta-llama/Llama-3.2-11B-Vision-Instruct:   0%|          | 0/3600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluating llava-hf/llava-v1.6-mistral-7b-hf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1277be2d84146f190daf3f43c4d7b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d09418400c64d589528bf67168cfba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llava-hf/llava-v1.6-mistral-7b-hf:   0%|          | 0/3600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluating OpenGVLab/InternVL2_5-8B-MPO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef87b991027c409487913727db1305df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4fb0573fd94a3590e33a58666f8f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "OpenGVLab/InternVL2_5-8B-MPO:   0%|          | 0/3600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4187817/2303040419.py:104: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '{}' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[index, model_name] = prediction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluating microsoft/Phi-3.5-vision-instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7c12acfa134c87b7def327587b7e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea63bec6b63d46e989028ac3805f39df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "microsoft/Phi-3.5-vision-instruct:   0%|          | 0/3600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    if model_name == \"Qwen/Qwen2-VL-7B-Instruct\":\n",
    "        model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_name, torch_dtype=\"auto\", device_map=\"auto\"\n",
    "        )\n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    elif model_name == \"google/paligemma-3b-pt-448\":\n",
    "        model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            token = get_huggingface_token()\n",
    "        ).eval()\n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "   \n",
    "    elif model_name == \"meta-llama/Llama-3.2-11B-Vision-Instruct\":\n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    elif model_name == \"llava-hf/llava-v1.6-mistral-7b-hf\":\n",
    "        model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            use_flash_attention_2=True,\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16) \n",
    "\n",
    "        processor = LlavaNextProcessor.from_pretrained(model_name)\n",
    "        processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "    elif model_name == \"OpenGVLab/InternVL2_5-8B-MPO\":\n",
    "        processor = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "        def split_model(model_name):\n",
    "            device_map = {}\n",
    "            world_size = torch.cuda.device_count()\n",
    "            num_layers = {\n",
    "                'InternVL2_5-1B': 24, 'InternVL2_5-2B': 24, 'InternVL2_5-4B': 36, 'InternVL2_5-8B': 32,\n",
    "                'InternVL2_5-26B': 48, 'InternVL2_5-38B': 64, 'InternVL2_5-78B': 80}[model_name]\n",
    "            # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "            num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "            num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "            num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "            layer_cnt = 0\n",
    "            for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "                for j in range(num_layer):\n",
    "                    device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "                    layer_cnt += 1\n",
    "            device_map['vision_model'] = 0\n",
    "            device_map['mlp1'] = 0\n",
    "            device_map['language_model.model.tok_embeddings'] = 0\n",
    "            device_map['language_model.model.embed_tokens'] = 0\n",
    "            device_map['language_model.output'] = 0\n",
    "            device_map['language_model.model.norm'] = 0\n",
    "            device_map['language_model.lm_head'] = 0\n",
    "            device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "            return device_map\n",
    "\n",
    "        device_map = split_model('InternVL2_5-8B')\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_flash_attn=True,\n",
    "            trust_remote_code=True,\n",
    "            device_map=device_map).eval()\n",
    "\n",
    "    elif model_name == \"microsoft/Phi-3.5-vision-instruct\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            device_map=\"cuda\", \n",
    "            trust_remote_code=True, \n",
    "            torch_dtype=\"auto\", \n",
    "            _attn_implementation='flash_attention_2'\n",
    "        )\n",
    "\n",
    "        # for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "        processor = AutoProcessor.from_pretrained(model_name,\n",
    "                                                  trust_remote_code=True,\n",
    "                                                  num_crops=16\n",
    "                                                  )\n",
    "    \n",
    "    # Iterate through the DataFrame rows\n",
    "    for index, row in tqdm(data.iterrows(), total=len(data), desc=f\"{model_name}\"):\n",
    "        image_path = row['image_path']\n",
    "        prompt = row['prompt']\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Get the model prediction for the image and prompt using model specific inference\n",
    "        prediction = get_model_prediction(model, processor, image, prompt) \n",
    "\n",
    "        # Store the prediction in the DataFrame\n",
    "        data.loc[index, model_name] = prediction\n",
    "\n",
    "    # delete model and free memory\n",
    "    del model\n",
    "    del processor\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store pandas dataframe\n",
    "data.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe from csv\n",
    "data = pd.read_csv(\"data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
