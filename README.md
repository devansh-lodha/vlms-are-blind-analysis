# vlms-are-blind-analysis
## Instructions to set up the project locally
```bash
python3 -m venv env
```
Activating the environment on Windows:
```bash
env\Scripts\activate
```
Activating the environment on MacOS/Linux:
```bash
source env/bin/activate
```
Installing dependencies:
```bash
pip3 install -r requirements.txt
```

## Data
The data is generated by [./line_intersection_data.ipynb](./line_intersection_data.ipynb). The data is stored in [./my2DlinePlots](./my2DlinePlots) folder (not pushed to github). It's the exact data as used for the paper.

## Evaluation
Evaluation code is setup in [./evaluation.ipynb](./evaluation.ipynb):

The prompts are again the same as used in the paper.
1. How many times do the blue and red lines touch each other? Answer with a number in curly brackets, e.g., {5}.
2. Count the intersection points where the blue and red lines meet. Put your answer in curly brackets, e.g., {2}.

The `get_model_predictions()` function implements inference code for all models. Then a loop is setup that loads the model and processor and evaluates the model on the test data. The model's predictions are stored in a pandas dataframe. All of the code is taken and modified from respective model's documentation on HuggingFace.

I've set up the code to run locally on Mac machines with MPS. To avoid memory issues, memory is freed after each model is evaluated.

The pandas dataframe will have the following columns for each of the 3600 rows:
- filename	
- gt
- linewidth
- resolution
- distances	
- image_path
- prompt
- Qwen/Qwen2-VL-7B-Instruct
- google/paligemma-3b-pt-448
- microsoft/Florence-2-large
- meta-llama/Llama-3.2-11B-Vision-Instruct
- llava-hf/llava-v1.6-mistral-7b-hf
- OpenGVLab/InternVL2_5-8B-MPO
- microsoft/Phi-3.5-vision-instruct
- mistralai/Pixtral-12B-2409

The model columns will store their predictions.

The 'instruct' variant models is chosen where available since they are optimized for VQA.

## Current Issues
- My request to test `google/paligemma2-3b-pt-224` isn't yet approved by google on HuggingFace.
- `microsoft/Florence-2-large` does not support VQA. Even the captions tend to include certain coordinates, but nothing about "number of interesections".
- I estimate that doing 3600 prompts for every model will take somewhere around 20-40 days on my machine (M3 Max, 14 Core CPU, 30 Core GPU, 36 GB RAM) which is not feasible. There are some possible solutions, we can either reduce controls for the data to reduce size or reduce the number of models we're testing on, or else I'd love to modify this code for cuda and multi-gpu support if I can get access to a powerful server.
- Some mac specific mps problems with `OpenGVLab/InternVL2_5-8B-MPO` (bicubic interpolation which is not yet supported for mps is used for resizing images), and `microsoft/Phi-3.5-vision-instruct`. I was unable to find a workaround for these issues and have resorted to CPU for these models. `mistralai/Pixtral-12B-2409` can only be implemented via vLLM which is primarily designed for high-performance inference on GPUs and not supported on MPS, so I'm not able to get this model to work.
- Model output is unpredictable, I've noticed that `meta-llama/Llama-3.2-11B-Vision-Instruct` tends to output text of different lengths sometimes with a number enclosed in {} or sometimes textual answers like "twice". Some sort of postprocessing is needed but I'm afraid its hard to make a generalization until we have a good number of model outputs. However, models like `Qwen/Qwen2-VL-7B-Instruct`, `OpenGVLab/InternVL2_5-8B-MPO`, `microsoft/Phi-3.5-vision-instruct` give answers nicely with minor variations like `2`, `{2}` or `[{2}]` which I've managed to fix by stripping in `get_model_predictions()` function. Similary `llava-hf/llava-v1.6-mistral-7b-hf` had a consistent output pattern which put the number in {}, so I simply did a regex search to get the output.

## What next
Once I have the complete pandas dataframe, I can compare the model predictions againt `gt` column to calucalte model-wise accuracy, accuracy by line-width, generate a heatmap with x-axis showing distance between plots and y axis with all models. All of these evaluations are inspired by what is shown in the paper.